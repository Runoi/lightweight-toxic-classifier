{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-24T19:53:14.011566Z",
     "iopub.status.busy": "2025-09-24T19:53:14.011300Z",
     "iopub.status.idle": "2025-09-24T19:53:25.108795Z",
     "shell.execute_reply": "2025-09-24T19:53:25.107889Z",
     "shell.execute_reply.started": "2025-09-24T19:53:14.011542Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install natasha -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-24T20:03:54.391608Z",
     "iopub.status.busy": "2025-09-24T20:03:54.391027Z",
     "iopub.status.idle": "2025-09-24T20:03:54.398843Z",
     "shell.execute_reply": "2025-09-24T20:03:54.398117Z",
     "shell.execute_reply.started": "2025-09-24T20:03:54.391585Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "from collections import Counter\n",
    "from tqdm.notebook import tqdm \n",
    "\n",
    "# --- Глобальные настройки ---\n",
    "# Пути для Kaggle\n",
    "DATA_PATH = '/kaggle/input/toxic-russian-comments-from-pikabu-and-2ch/russian_comments_from_2ch_pikabu.csv'\n",
    "WORKING_DIR = '/kaggle/working/'\n",
    "\n",
    "# Параметры модели\n",
    "VOCAB_SIZE = 20000      # Размер словаря (увеличено для более качественного словаря)\n",
    "EMBED_DIM = 256         # Размерность эмбеддингов\n",
    "MAX_SEQ_LEN = 128       # Максимальная длина последовательности\n",
    "NUM_FILTERS = 256       # Количество сверточных фильтров\n",
    "KERNEL_SIZES = [3, 4, 5]# Размеры ядер сверток\n",
    "NUM_CLASSES = 2\n",
    "DROPOUT_RATE = 0.3\n",
    "\n",
    "# Параметры обучения\n",
    "BATCH_SIZE = 64         \n",
    "LEARNING_RATE = 1e-5\n",
    "EPOCHS = 20             \n",
    "\n",
    "# Выбор устройства (GPU, если доступно)\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Используется устройство: {DEVICE}\")\n",
    "\n",
    "# Пути для сохранения артефактов\n",
    "VOCAB_PATH = os.path.join(WORKING_DIR, 'vocab.json')\n",
    "MODEL_FP32_PATH = os.path.join(WORKING_DIR, 'solo_cnn_fp32.pth')\n",
    "MODEL_INT8_PATH = os.path.join(WORKING_DIR, 'solo_cnn_int8.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-24T19:53:53.807256Z",
     "iopub.status.busy": "2025-09-24T19:53:53.806885Z",
     "iopub.status.idle": "2025-09-24T19:53:54.699962Z",
     "shell.execute_reply": "2025-09-24T19:53:54.699221Z",
     "shell.execute_reply.started": "2025-09-24T19:53:53.807237Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(DATA_PATH)\n",
    "df.dropna(subset=['comment', 'toxic'], inplace=True)\n",
    "df['toxic'] = df['toxic'].astype(int)\n",
    "df = df.rename(columns={'toxic': 'label'}) # Переименуем для единообразия\n",
    "\n",
    "# Разделяем данные\n",
    "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['label'])\n",
    "print(f\"Размер обучающей выборки: {len(train_df)}\")\n",
    "print(f\"Размер валидационной выборки: {len(val_df)}\")\n",
    "\n",
    "# Рассчитываем веса классов для борьбы с дисбалансом\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(train_df['label']),\n",
    "    y=train_df['label'].to_numpy()\n",
    ")\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float).to(DEVICE)\n",
    "print(f\"\\nРассчитанные веса для классов [0, 1]: {class_weights_tensor}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-24T19:53:56.567301Z",
     "iopub.status.busy": "2025-09-24T19:53:56.567022Z",
     "iopub.status.idle": "2025-09-24T19:54:00.151027Z",
     "shell.execute_reply": "2025-09-24T19:54:00.150418Z",
     "shell.execute_reply.started": "2025-09-24T19:53:56.567279Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from natasha import (Segmenter, Doc)\n",
    "import re\n",
    "from collections import Counter\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "segmenter = Segmenter()\n",
    "\n",
    "def robust_tokenizer(text):\n",
    "    \"\"\"\n",
    "    Надежная токенизация:\n",
    "    1. Правильное разделение на токены с помощью Natasha.\n",
    "    2. Очистка от мусора (не-слов).\n",
    "    3. Приведение к нижнему регистру.\n",
    "    Лемматизация НЕ используется для максимальной надежности.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str) or not text.strip(): \n",
    "        return []\n",
    "    \n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'\\@\\w+', '', text)\n",
    "    \n",
    "    doc = Doc(text)\n",
    "    doc.segment(segmenter)\n",
    "    \n",
    "    # Просто берем текст токена, если это слово, и приводим к нижнему регистру\n",
    "    tokens = [\n",
    "        token.text.lower() for token in doc.tokens \n",
    "        if token.text.isalpha() # Оставляем только токены, состоящие из букв\n",
    "    ]\n",
    "            \n",
    "    return tokens\n",
    "\n",
    "def build_vocab_robust(texts, vocab_size):\n",
    "    token_counts = Counter()\n",
    "    for text in tqdm(texts, desc=\"Построение словаря\"):\n",
    "        token_counts.update(robust_tokenizer(text))\n",
    "        \n",
    "    vocab = {'<pad>': 0, '<unk>': 1}\n",
    "    for token, _ in token_counts.most_common(vocab_size - 2):\n",
    "        vocab[token] = len(vocab)\n",
    "    return vocab\n",
    "\n",
    "print(\"Построение словаря на обучающих данных (надежный режим)...\")\n",
    "# Убедитесь, что в Ячейке 2 вы переименовали 'comment' в 'text'\n",
    "vocab = build_vocab_robust(train_df['comment'].to_list(), VOCAB_SIZE)\n",
    "print(f\"Размер построенного словаря: {len(vocab)}\")\n",
    "\n",
    "if len(vocab) < 10:\n",
    "    print(\"\\n!!! КРИТИЧЕСКАЯ ОШИБКА: Словарь все еще пуст.\")\n",
    "else:\n",
    "    print(\"\\nСловарь успешно построен.\")\n",
    "    with open(VOCAB_PATH, 'w', encoding='utf-8') as f:\n",
    "        json.dump(vocab, f, ensure_ascii=False, indent=4)\n",
    "    print(f\"Словарь сохранен в: {VOCAB_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-24T20:04:04.291113Z",
     "iopub.status.busy": "2025-09-24T20:04:04.290532Z",
     "iopub.status.idle": "2025-09-24T20:04:04.302489Z",
     "shell.execute_reply": "2025-09-24T20:04:04.301679Z",
     "shell.execute_reply.started": "2025-09-24T20:04:04.291088Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def tokenize_text_robust(text, vocab, max_len):\n",
    "    # Вызываем правильную функцию\n",
    "    tokens = robust_tokenizer(text)\n",
    "    \n",
    "    # Преобразуем токены в ID\n",
    "    indexed_tokens = [vocab.get(token, vocab['<unk>']) for token in tokens]\n",
    "    \n",
    "    # Обрезка или дополнение (Padding)\n",
    "    padding = [vocab['<pad>']] * (max_len - len(indexed_tokens))\n",
    "    indexed_tokens = indexed_tokens[:max_len] + padding[:max(0, max_len - len(indexed_tokens))]\n",
    "    \n",
    "    return torch.tensor(indexed_tokens, dtype=torch.long)\n",
    "\n",
    "class SoloCNNDataset(Dataset):\n",
    "    def __init__(self, texts, labels, vocab, max_len):\n",
    "        self.texts = texts\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "        self.vocab = vocab\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self): \n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        return {\n",
    "            'input_ids': tokenize_text_robust(text, self.vocab, self.max_len),\n",
    "            'labels': self.labels[idx]\n",
    "        }\n",
    "\n",
    "\n",
    "train_dataset = SoloCNNDataset(train_df['comment'].to_list(), train_df['label'].to_list(), vocab, MAX_SEQ_LEN)\n",
    "val_dataset = SoloCNNDataset(val_df['comment'].to_list(), val_df['label'].to_list(), vocab, MAX_SEQ_LEN)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "print(\"DataLoader'ы успешно созданы и синхронизированы с Ячейкой 3.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-24T20:30:10.618052Z",
     "iopub.status.busy": "2025-09-24T20:30:10.617306Z",
     "iopub.status.idle": "2025-09-24T20:30:10.674482Z",
     "shell.execute_reply": "2025-09-24T20:30:10.673657Z",
     "shell.execute_reply.started": "2025-09-24T20:30:10.618028Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class SoloCNNTextClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_filters, kernel_sizes, num_classes, dropout_rate, pad_idx=0):\n",
    "        super(SoloCNNTextClassifier, self).__init__()\n",
    "        \n",
    "        self.quant = torch.quantization.QuantStub()\n",
    "        self.dequant = torch.quantization.DeQuantStub()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_idx)\n",
    "        \n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Conv1d(in_channels=embed_dim, out_channels=num_filters, kernel_size=k),\n",
    "                nn.BatchNorm1d(num_filters),\n",
    "                nn.ReLU(inplace=True)\n",
    "            )\n",
    "            for k in kernel_sizes\n",
    "        ])\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc = nn.Linear(len(kernel_sizes) * num_filters, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 1. Мир FP32: Embedding работает с float\n",
    "        embedded = self.embedding(x).permute(0, 2, 1)\n",
    "        \n",
    "        # 2. Пересекаем \"мост\" в мир INT8\n",
    "        quantized_embedded = self.quant(embedded)\n",
    "        \n",
    "        # 3. Мир INT8: Все операции здесь будут квантованы\n",
    "        # Используем квантованный тензор на входе\n",
    "        conved = [conv(quantized_embedded) for conv in self.convs]\n",
    "        \n",
    "        pooled = [F.max_pool1d(c, c.shape[2]).squeeze(2) for c in conved]\n",
    "        concatenated = self.dropout(torch.cat(pooled, dim=1))\n",
    "        \n",
    "        # 4. Проходим через последний квантованный слой (Linear)\n",
    "        quantized_logits = self.fc(concatenated)\n",
    "        \n",
    "        # 5. Пересекаем \"мост\" обратно в мир FP32 для вывода\n",
    "        output_logits = self.dequant(quantized_logits)\n",
    "        \n",
    "        return output_logits\n",
    "\n",
    "model = SoloCNNTextClassifier(\n",
    "    vocab_size=len(vocab), embed_dim=EMBED_DIM, num_filters=NUM_FILTERS,\n",
    "    kernel_sizes=KERNEL_SIZES, num_classes=NUM_CLASSES, dropout_rate=DROPOUT_RATE\n",
    ").to(DEVICE)\n",
    "\n",
    "print(model)\n",
    "print(f\"\\nКоличество обучаемых параметров: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-24T20:30:10.676515Z",
     "iopub.status.busy": "2025-09-24T20:30:10.675891Z",
     "iopub.status.idle": "2025-09-24T20:30:10.704702Z",
     "shell.execute_reply": "2025-09-24T20:30:10.703903Z",
     "shell.execute_reply.started": "2025-09-24T20:30:10.676496Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"--- Проверка одного батча из train_loader ---\")\n",
    "# Получаем один батч данных\n",
    "sample_batch = next(iter(train_loader))\n",
    "input_ids = sample_batch['input_ids']\n",
    "labels = sample_batch['labels']\n",
    "\n",
    "print(f\"Размер тензора input_ids: {input_ids.shape}\")\n",
    "print(f\"Размер тензора labels: {labels.shape}\")\n",
    "print(f\"Примеры меток в батче: {labels[:10]}\")\n",
    "print(f'Соотношение классов в батче (примерно): {labels.float().mean():.2f} (0=нетокс, 1=токс)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-24T20:30:10.705616Z",
     "iopub.status.busy": "2025-09-24T20:30:10.705380Z",
     "iopub.status.idle": "2025-09-24T20:32:48.531182Z",
     "shell.execute_reply": "2025-09-24T20:32:48.530435Z",
     "shell.execute_reply.started": "2025-09-24T20:30:10.705600Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "criterion = nn.CrossEntropyLoss() # Оставляем без весов\n",
    "\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=len(train_loader) * EPOCHS)\n",
    "\n",
    "best_f1_score = 0.0\n",
    "GRADIENT_CLIP_VALUE = 1.0\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    train_pbar = tqdm(train_loader, desc=f\"Эпоха {epoch}/{EPOCHS} [Обучение]\")\n",
    "    for batch in train_pbar:\n",
    "        input_ids, labels = batch['input_ids'].to(DEVICE), batch['labels'].to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(input_ids)\n",
    "        loss = criterion(logits, labels)\n",
    "        total_train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), GRADIENT_CLIP_VALUE)\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Шаг планировщика после каждого батча\n",
    "        scheduler.step()\n",
    "        \n",
    "        train_pbar.set_postfix({'loss': loss.item(), 'lr': scheduler.get_last_lr()[0]})\n",
    "\n",
    "    \n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        val_pbar = tqdm(val_loader, desc=f\"Эпоха {epoch}/{EPOCHS} [Валидация]\")\n",
    "        for batch in val_pbar:\n",
    "            input_ids, labels = batch['input_ids'].to(DEVICE), batch['labels'].to(DEVICE)\n",
    "            logits = model(input_ids)\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    report = classification_report(all_labels, all_preds, output_dict=True, zero_division=0)\n",
    "    f1_toxic = report['1']['f1-score']\n",
    "    print(f\"\\nЭпоха {epoch}: Train Loss: {total_train_loss / len(train_loader):.4f}, F1-score (Токсичный): {f1_toxic:.4f}\")\n",
    "\n",
    "    if f1_toxic > best_f1_score:\n",
    "        best_f1_score = f1_toxic\n",
    "        torch.save(model.state_dict(), MODEL_FP32_PATH)\n",
    "        print(f\"  -> Новая лучшая модель сохранена! F1-score: {best_f1_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-24T20:32:48.533213Z",
     "iopub.status.busy": "2025-09-24T20:32:48.532926Z",
     "iopub.status.idle": "2025-09-24T20:32:49.722070Z",
     "shell.execute_reply": "2025-09-24T20:32:49.721248Z",
     "shell.execute_reply.started": "2025-09-24T20:32:48.533195Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Загружаем лучшую модель\n",
    "model.load_state_dict(torch.load(MODEL_FP32_PATH))\n",
    "model.to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "all_preds, all_labels = [], []\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(val_loader, desc=\"Финальная оценка\"):\n",
    "        input_ids, labels = batch['input_ids'].to(DEVICE), batch['labels'].to(DEVICE)\n",
    "        logits = model(input_ids)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "print(\"\\n--- Отчет по классификации для лучшей FP32 модели ---\")\n",
    "print(classification_report(all_labels, all_preds, target_names=['Нетоксичный (0)', 'Токсичный (1)']))\n",
    "\n",
    "print(\"\\n--- Матрица ошибок ---\")\n",
    "# [[TN, FP],\n",
    "#  [FN, TP]]\n",
    "print(confusion_matrix(all_labels, all_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-24T20:34:21.231972Z",
     "iopub.status.busy": "2025-09-24T20:34:21.231104Z",
     "iopub.status.idle": "2025-09-24T20:34:23.007302Z",
     "shell.execute_reply": "2025-09-24T20:34:23.006655Z",
     "shell.execute_reply.started": "2025-09-24T20:34:21.231946Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch.quantization\n",
    "from torch.ao.quantization import QConfigMapping, float_qparams_weight_only_qconfig\n",
    "\n",
    "# 1. Загружаем FP32 модель на CPU\n",
    "cpu_model = SoloCNNTextClassifier(\n",
    "    vocab_size=len(vocab), embed_dim=EMBED_DIM, num_filters=NUM_FILTERS,\n",
    "    kernel_sizes=KERNEL_SIZES, num_classes=NUM_CLASSES, dropout_rate=DROPOUT_RATE\n",
    ")\n",
    "cpu_model.load_state_dict(torch.load(MODEL_FP32_PATH, map_location=\"cpu\"))\n",
    "\n",
    "# 2. Переводим модель в режим инференса (ОБЯЗАТЕЛЬНО перед слиянием)\n",
    "cpu_model.eval()\n",
    "\n",
    "# Мы проходим по каждому нашему сверточному блоку и сливаем Conv, BN и ReLU в один модуль.\n",
    "print(\"Слияние модулей (fusing)...\")\n",
    "for conv_module in cpu_model.convs:\n",
    "    # Имена '0', '1', '2' соответствуют порядку слоев в nn.Sequential\n",
    "    torch.quantization.fuse_modules(conv_module, ['0', '1', '2'], inplace=True)\n",
    "print(\"Слияние завершено.\")\n",
    "\n",
    "# 3. Настраиваем конфигурацию квантизации (как и раньше)\n",
    "cpu_model.qconfig = torch.quantization.get_default_qconfig('fbgemm')\n",
    "cpu_model.embedding.qconfig = torch.quantization.float_qparams_weight_only_qconfig\n",
    "\n",
    "# 4. Готовим модель к калибровке. Теперь она увидит слитые модули.\n",
    "torch.quantization.prepare(cpu_model, inplace=True)\n",
    "\n",
    "# 5. Калибровка (остается без изменений)\n",
    "print(\"Калибровка модели...\")\n",
    "with torch.no_grad():\n",
    "    train_loader_iter = iter(train_loader)\n",
    "    for i in range(10): \n",
    "        try:\n",
    "            batch = next(train_loader_iter)\n",
    "            cpu_model(batch['input_ids'])\n",
    "        except StopIteration:\n",
    "            break\n",
    "\n",
    "# 6. Конвертация (остается без изменений)\n",
    "torch.quantization.convert(cpu_model, inplace=True)\n",
    "print(\"Модель успешно квантована!\")\n",
    "\n",
    "# 7. Сохраняем (остается без изменений)\n",
    "scripted_quantized_model = torch.jit.script(cpu_model)\n",
    "torch.jit.save(scripted_quantized_model, MODEL_INT8_PATH)\n",
    "print(f\"Квантованная INT8 модель сохранена в: {MODEL_INT8_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-24T20:34:26.162937Z",
     "iopub.status.busy": "2025-09-24T20:34:26.162147Z",
     "iopub.status.idle": "2025-09-24T20:34:26.248951Z",
     "shell.execute_reply": "2025-09-24T20:34:26.248046Z",
     "shell.execute_reply.started": "2025-09-24T20:34:26.162911Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Загружаем артефакты (модель и словарь)\n",
    "quantized_model = torch.jit.load(MODEL_INT8_PATH)\n",
    "quantized_model.eval()\n",
    "\n",
    "with open(VOCAB_PATH, 'r', encoding='utf-8') as f:\n",
    "    loaded_vocab = json.load(f)\n",
    "\n",
    "# Создаем функцию-пайплайн для предсказания\n",
    "def predict_toxicity(text, model, vocab, max_len=128, threshold=0.5):\n",
    "    input_ids = tokenize_text_robust(text, vocab, max_len).unsqueeze(0)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        logits = model(input_ids)\n",
    "        probabilities = F.softmax(logits, dim=1).squeeze()\n",
    "    \n",
    "    score_toxic = probabilities[1].item()\n",
    "    is_toxic = score_toxic > threshold\n",
    "    \n",
    "    return {\n",
    "        \"text\": text,\n",
    "        \"is_toxic\": is_toxic,\n",
    "        \"toxic_score\": f\"{score_toxic:.4f}\"\n",
    "    }\n",
    "\n",
    "# Тестовые предложения\n",
    "test_sentences = [\n",
    "    \"какой прекрасный сегодня день, желаю всем счастья!\",\n",
    "    \"ты ведешь себя как полный дурак и придурок\",\n",
    "    \"Отличная работа, команда!\",\n",
    "    \"автор, ты неправ и пишешь какую-то чушь\",\n",
    "    \"Это просто худший сервис из всех, что я пробовал.\"\n",
    "]\n",
    "\n",
    "print(\"\\n--- Тест инференса на новых данных ---\")\n",
    "for sentence in test_sentences:\n",
    "    prediction = predict_toxicity(sentence, quantized_model, loaded_vocab)\n",
    "    print(prediction)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 705205,
     "sourceId": 1231679,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
